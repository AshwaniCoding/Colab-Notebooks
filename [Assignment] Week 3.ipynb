{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Week 3 Assignment: Building an Advanced RAG System**\n",
        "---\n",
        "\n",
        "### **Objective**\n",
        "\n",
        "The goal of this assignment is to build, evaluate, and iteratively improve a Retrieval-Augmented Generation (RAG) system using a state-of-the-art Large Language Model from Google's Gemini family. You will move beyond a basic pipeline to implement advanced techniques like reranking, with the final application answering complex questions from a real-world financial document.\n",
        "\n",
        "### **Problem Statement**\n",
        "\n",
        "You are an AI Engineer at a top financial services firm. Your team has been tasked with creating a tool to help financial analysts quickly extract key information from lengthy, complex annual reports (10-K filings). Manually searching these 100+ page documents for specific figures or risk assessments is slow and error-prone.\n",
        "\n",
        "Your task is to build a RAG-based Q&A system that allows an analyst to ask natural language questions about a company's 10-K report and receive accurate, grounded answers powered by Gemini.\n",
        "\n",
        "### **Dataset**\n",
        "\n",
        "You will be using the official 2022 10-K annual report for **Microsoft**. A 10-K report is a comprehensive summary of a company's financial performance.\n",
        "*   **Download Link:** [Microsoft Corp. 2022 10-K Report (PDF)](https://www.sec.gov/Archives/edgar/data/789019/000156459022026876/msft-10k_20220630.htm)\n",
        "    *   *Instructions: Go to the link, and save the webpage as a `.txt` file or copy-paste the relevant sections into a text file for easier processing.*\n",
        "\n",
        "---\n",
        "\n",
        "### **Tasks & Instructions**\n",
        "\n",
        "Structure your work in a Jupyter Notebook (`.ipynb`) or Python files. Use markdown cells or comments (in case of Python file-based submissions) to explain your methodology, justify your choices, and present your findings at each stage.\n",
        "\n",
        "**Part 1: Setup and API Configuration**\n",
        "*   **Objective:** To configure your environment to use the Google Gemini API (or an equivalent model).\n",
        "*   **Tasks:**\n",
        "    1.  **Get Your API Key:**\n",
        "        *   Go to [Google AI Studio](https://aistudio.google.com/).\n",
        "        *   Sign in with your Google account.\n",
        "        *   Click on **\"Get API key\"** and create a new API key. **Treat this key like a password and do not share it publicly.**\n",
        "    2.  **Environment Setup:**\n",
        "        *   In your development environment (for example, Google Colab notebook or VSCode on your local machine), install the necessary libraries: `pip install -q -U google-generativeai langchain-google-genai langchain chromadb sentence-transformers`.\n",
        "        *   If you're using Colab, use the \"Secrets\" feature (look for the key icon ðŸ”‘ on the left sidebar) to securely store your API key. Create a new secret named `GEMINI_API_KEY` and paste your key there.\n",
        "    3.  **Configure the LLM:** In your code, import the necessary libraries and configure your LLM. For example, if you're using Colab:\n",
        "        ```python\n",
        "        import google.generativeai as genai\n",
        "        from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "        from google.colab import userdata\n",
        "\n",
        "        # Configure the API key\n",
        "        api_key = userdata.get('GEMINI_API_KEY')\n",
        "        genai.configure(api_key=api_key)\n",
        "\n",
        "        # Instantiate the Gemini model\n",
        "        llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
        "        ```\n",
        "\n",
        "**Part 2: Building the Baseline RAG System**\n",
        "*   **Objective:** To construct a standard, vector-search-only RAG pipeline using Gemini (or an equivalent model) as the generator.\n",
        "*   **Tasks:**\n",
        "    1.  **Document Loading:** Load the Microsoft 10-K report into your application.\n",
        "    2.  **Chunking:** Split the document into chunks. **In a markdown cell (or in a comment, if using Python instead of Jupyter), explicitly state your chosen `chunk_size` and `chunk_overlap` and briefly explain why you chose those values.**\n",
        "    3.  **Vector Store:** Create embeddings for your chunks using an open-source model (e.g., `sentence-transformers/all-MiniLM-L6-v2`) and store them in a vector database (e.g., ChromaDB).\n",
        "    4.  **QA Chain:** Create a standard `RetrievalQA` chain using the `llm` object (Gemini 2.5 Flash or equivalent) you configured in Part 1.\n",
        "    5.  **Initial Test:** Test your baseline system with the following question: `\"What were the company's total revenues for the fiscal year that ended on June 30, 2022?\"`. Display the answer.\n",
        "\n",
        "**Part 3: Evaluating the Baseline**\n",
        "*   **Objective:** To quantitatively and qualitatively assess the performance of your LLM-powered system.\n",
        "*   **Tasks:**\n",
        "    1.  **Create a Test Set:** Create a small evaluation set of at least **five** questions. These questions should be a mix of:\n",
        "        *   **Specific Fact Retrieval:** (e.g., \"What is the name of the company's independent registered public accounting firm?\")\n",
        "        *   **Summarization:** (e.g., \"Summarize the key risks related to competition.\")\n",
        "        *   **Keyword-Dependent:** (e.g., \"What does the report say about 'Azure'?\")\n",
        "    2.  **Qualitative Evaluation:** Run your five questions through the baseline RAG system. For each question, display the generated answer and the source chunks that were retrieved.\n",
        "    3.  **Analysis:** In a markdown cell (or in a comment, if using Python instead of Jupyter), write a brief analysis. Did the system answer correctly? Were the retrieved chunks relevant? Did you notice any failures?\n",
        "\n",
        "**Part 4: Implementing an Advanced RAG Technique**\n",
        "*   **Objective:** To improve upon the baseline by implementing a reranker.\n",
        "*   **Tasks:**\n",
        "    1.  **Implement a Reranker:** Add a reranker (e.g., using `CohereRerank` or a Hugging Face cross-encoder model) into your pipeline. The flow should be: Retrieve top 10 docs -> Rerank to get the best 3 -> Pass only these 3 to LLM for the final answer.\n",
        "    2.  **Re-Evaluation:** Run your same five evaluation questions through your new, advanced RAG pipeline. Display the generated answer and the final source chunks for each.\n",
        "\n",
        "**Part 5: Final Analysis and Conclusion**\n",
        "*   **Objective:** To compare the baseline and advanced systems and articulate the value of the advanced technique.\n",
        "*   **Tasks:**\n",
        "    1.  **Comparison:** In a markdown cell (or in a comment, if using Python instead of Jupyter), create a simple table or a structured list comparing the answers from the **Baseline RAG** vs. the **Advanced RAG** for your five evaluation questions.\n",
        "    2.  **Conclusion:** Write a concluding paragraph answering the following:\n",
        "        *   Did adding the reranker improve the results? How?\n",
        "        *   Based on your experience, what is the biggest challenge in building a reliable RAG system for dense documents?\n",
        "\n",
        "**Bonus Section (Optional)**\n",
        "*   **Objective:** To demonstrate a deeper understanding by implementing more complex features.\n",
        "*   **Choose any of the following to implement:**\n",
        "    *   **Implement Query Rewriting:** Before the retrieval step, use Gemini itself to rewrite the user's query to be more effective for a financial document.\n",
        "    *   **Automated Evaluation with RAGAS:** Use the `ragas` library to automatically score the faithfulness and relevance of your baseline vs. your advanced system.\n",
        "    *   **Source Citing:** Modify your pipeline to not only return the answer but also explicitly cite the source chunk(s) it used.\n",
        "\n",
        "---\n",
        "\n",
        "### **Submission Instructions**\n",
        "\n",
        "1.  **Deadline:** You have **two weeks** from the assignment release date to submit your work.\n",
        "2.  **Platform:** All submissions must be made to your allocated private GitLab repository. You **must** submit your work in a branch named `week_3`.\n",
        "3.  **Format:** You can submit your work as either a Jupyter Notebook (`.ipynb`) or a collection of Python scripts (`.py`).\n",
        "4.  After pushing, you should verify that your branch and files are visible on the GitLab web interface. No further action is needed. The trainers will review all submissions on the `week_3` branch after the deadline. Any assignments submitted after the deadline won't be reviewed and will reflect in your course score.\n",
        "5. The use of LLMs is encouraged, but ensure that youâ€™re not copying solutions blindly. Always review, test, and understand any code generated, adapting it to the specific requirements of your assignment. Your submission should demonstrate your own comprehension, problem-solving process, and coding style, not just an unedited output from an AI tool."
      ],
      "metadata": {
        "id": "NraGy9wtq-9M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1 - Setup"
      ],
      "metadata": {
        "id": "AG9enWw-TJmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required (1.37.0) versions to satisfy existing dependencies\n",
        "!pip install -q opentelemetry-api==1.37.0 opentelemetry-sdk==1.37.0 opentelemetry-proto==1.37.0 opentelemetry-exporter-otlp-proto-common==1.37.0 opentelemetry-exporter-otlp-proto-grpc==1.37.0 requests==2.32.5"
      ],
      "metadata": {
        "id": "LBSOFdWKX3Yi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U google-generativeai langchain-google-genai langchain chromadb sentence-transformers langchain_community"
      ],
      "metadata": {
        "id": "9bp484k7KeUj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# Configure the API key\n",
        "api_key = userdata.get('GEMINI_API_KEY')\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "# Instantiate the Gemini model\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
        "\n",
        "print(\"âœ… Gemini model configured successfully!\")"
      ],
      "metadata": {
        "id": "2MjtsmquKh2h",
        "outputId": "2c2f6188-9914-4f4f-9a81-2039d29d3c54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Gemini model configured successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2 - Building the Baseline RAG System"
      ],
      "metadata": {
        "id": "InHDjX6ZKlig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Document Loading"
      ],
      "metadata": {
        "id": "3gsthTizMZKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/sample_data/msft-10K-data.txt\"\n",
        "\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    text_data = f.read()\n",
        "\n",
        "print(\"âœ… Microsoft 10-K file loaded successfully!\")\n",
        "print(f\"Total characters in document: {len(text_data)}\\n\")"
      ],
      "metadata": {
        "id": "53pnN5LiMKJB",
        "outputId": "c236d639-e049-4a56-ad48-e461329f2b2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Microsoft 10-K file loaded successfully!\n",
            "Total characters in document: 392586\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Chunking"
      ],
      "metadata": {
        "id": "A36NrnbYMckl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "chunk_size = 1200\n",
        "chunk_overlap = 200\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=chunk_size,\n",
        "    chunk_overlap=chunk_overlap,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        ")\n",
        "\n",
        "texts = text_splitter.split_text(text_data)\n",
        "print(f\"âœ… Total chunks created: {len(texts)}\")"
      ],
      "metadata": {
        "id": "yUtz6u3fMfdq",
        "outputId": "788c98bd-4c5e-493d-d145-e0f256a529bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Total chunks created: 443\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Vector Store"
      ],
      "metadata": {
        "id": "uPdGg5UQMjjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "embed_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embedding_function = SentenceTransformerEmbeddings(model_name=embed_model_name)\n",
        "\n",
        "persist_directory = \"./msft_chroma_store\"\n",
        "\n",
        "vectordb = Chroma.from_texts(\n",
        "    texts=texts,\n",
        "    embedding=embedding_function,\n",
        "    persist_directory=persist_directory\n",
        ")\n",
        "vectordb.persist()\n",
        "\n",
        "print(\"âœ… Embeddings created and stored in ChromaDB.\\n\")"
      ],
      "metadata": {
        "id": "e2umItYjMoD8",
        "outputId": "467b24d6-3b07-451a-81da-6b094bd678e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-80573503.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_function = SentenceTransformerEmbeddings(model_name=embed_model_name)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Embeddings created and stored in ChromaDB.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-80573503.py:14: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
            "  vectordb.persist()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### QA Chain"
      ],
      "metadata": {
        "id": "snBFIK_0PBTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "retriever = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,                     # Gemini 2.5 Flash\n",
        "    chain_type=\"stuff\",          # simplest type\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "print(\"âœ… QA chain ready!\\n\")"
      ],
      "metadata": {
        "id": "FuzVgFr9PD9m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "038223bb-0a9a-4dd3-8e16-b9013f309c80"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… QA chain ready!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Initial Test"
      ],
      "metadata": {
        "id": "Kqz_LpjqPGKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What were the company's total revenues for the fiscal year that ended on June 30, 2022?\"\n",
        "\n",
        "result = qa_chain.invoke({\"query\": query})\n",
        "\n",
        "# Display the answer\n",
        "print(\"=== ðŸ’¬ MODEL ANSWER ===\")\n",
        "print(result[\"result\"])\n",
        "print(\"\\n\")\n",
        "\n",
        "# Show the source snippets used by the model\n",
        "print(\"=== ðŸ“„ SOURCE DOCUMENTS ===\")\n",
        "for i, doc in enumerate(result[\"source_documents\"], 1):\n",
        "    print(f\"\\n--- Source {i} ---\")\n",
        "    print(doc.page_content[:400].replace(\"\\n\", \" \"), \"...\")"
      ],
      "metadata": {
        "id": "rydGmEw-PJfL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "c4a90b4b-137a-4b45-c3af-f69f53d896e4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Missing some input keys: {'query'}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2009110448.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"What were the company's total revenues for the fiscal year that ended on June 30, 2022?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqa_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"input\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Display the answer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         )\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m             outputs = (\n\u001b[1;32m    165\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m_validate_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmissing_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing some input keys: {missing_keys}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Missing some input keys: {'query'}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating the Baseline"
      ],
      "metadata": {
        "id": "DUADc8S-R4sV"
      }
    }
  ]
}